attn_dropout: 0.1
causal: false
decay_rate: 0.95
depth: 6
dim: 30
dim_head: 16
emb_dropout: 0.1
feature_redraw_interval: 100
ff_chunks: 10
ff_dropout: 0.1
ff_glu: true
generalized_attention: true
heads: 6
kernel_fn: !!python/object:torch.nn.modules.activation.ReLU
  _backward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _buffers: !!python/object/apply:collections.OrderedDict
  - []
  _forward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _forward_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _is_full_backward_hook: null
  _load_state_dict_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _modules: !!python/object/apply:collections.OrderedDict
  - []
  _non_persistent_buffers_set: !!set {}
  _parameters: !!python/object/apply:collections.OrderedDict
  - []
  _state_dict_hooks: !!python/object/apply:collections.OrderedDict
  - []
  inplace: false
  training: true
local_attn_heads: 4
local_window_size: 256
lr: 0.001
mask_frac: 0.85
max_seq_len: 25000
nb_features: 80
num_tokens: 7
rand_frac: 0.1
reversible: true
tie_embed: false
use_rezero: false
use_scalenorm: false
